{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KurlenMurlen/BigDataAssignment2/blob/main/TDE3_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhbCeS5VOlca"
      },
      "source": [
        "# TDE 3 - Spark Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIwrJWzmN6oa"
      },
      "source": [
        "You and your team were hired to process data using Apache Spark. The team have to answer the 10 (ten) questions (tasks) defined in the activity TDE 2, 3, and 4 - Dataset Definition.\n",
        "\n",
        "Given the aforementioned context, you are in charge of developing a set of solutions that allow the company to answer the following demands:\n",
        "\n",
        "- (Easy) The first task evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "- (Easy) The second task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "- (Easy) The third task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "- (Easy) The fourth task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "- (Medium) The first task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "- (Medium) The second task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "- (Medium) The third task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "- (Medium) The fourth task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "- (Hard) The first task Containing the use of multiple jobs, Custom Keys, Custom Values, Combining to solve the tasks;\n",
        "\n",
        "- (Hard) The second task Containing the use of multiple jobs, Custom Keys, Custom Values, Combining to solve the tasks;\n",
        "\n",
        "Given your knowledge and skills in Python and Apache Spark, for each item above, provide:\n",
        "\n",
        "- The source code for solving the problem using Apache Spark programming\n",
        "\n",
        "- The result of your code run in a separate text file (.txt). If more than 5 rows of results are available, you must report only the 5 first rows of such result.\n",
        "\n",
        "Important:\n",
        "\n",
        "- The grading of this activity is conditioned to the audit test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFVOzVoGOUi7"
      },
      "source": [
        "# Group:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ncE1HZ7O_ZS"
      },
      "source": [
        "- Murilo Chandelier Pedrazzani\n",
        "- Ricardo Ryu Magalhães Makino\n",
        "- Tarso Betolini Rodriguês"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw_j7E6MOwNa"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPX5yDxTPFDr"
      },
      "source": [
        "- Suicidios entre 2010 a 2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrn7FmkWamAZ"
      },
      "source": [
        "# Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIdZuaG0ao15"
      },
      "source": [
        "0. \" \"\n",
        "1. \"estado\"\n",
        "2. \"ano\"\n",
        "3. \"mes\"\n",
        "4. \"DTOBITO\"\n",
        "5. \"DTNASC\"\n",
        "6. \"SEXO\"\n",
        "7. \"RACACOR\"\n",
        "8. \"ASSISTMED\"\n",
        "9. \"ESCMAE\"\n",
        "10. \"ESTCIV\"\n",
        "11. \"ESC\"\n",
        "12. \"OCUP\"\n",
        "13. \"CODMUNRES\"\n",
        "14. \"CAUSABAS\"\n",
        "15. \"CAUSABAS_O\"\n",
        "16. \"LOCOCOR\"\n",
        "17. \"CIRURGIA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_6x3PBrNdkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943cae79-030c-46e1-fd8b-7a5d96b3650d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vyyiOpgo3J4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import findspark\n",
        "findspark.init()\n",
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe3FZAs4bvBA"
      },
      "outputs": [],
      "source": [
        "# Arquivo csv para o dataset\n",
        "dataset_file = \"suicidios_2010_a_2019.csv\"\n",
        "\n",
        "# Função para remover diretórios existentes\n",
        "def remove_dir(output_dir):\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "\n",
        "# Função para dividir cada linha usando o leitor de CSV do Python\n",
        "def split_csv_line(line):\n",
        "    # Usa o csv.reader para dividir a linha, respeitando as aspas\n",
        "    reader = csv.reader(StringIO(line), delimiter=\",\")\n",
        "    return next(reader)\n",
        "\n",
        "# Configura e cria o contexto do Spark\n",
        "def create_spark_context(app_name):\n",
        "    conf = SparkConf().setAppName(app_name).setMaster(\"local\")\n",
        "    return SparkContext.getOrCreate(conf)\n",
        "\n",
        "# Carrega o arquivo e remove o cabeçalho\n",
        "def load_data(dataset_file):\n",
        "    rdd = sc.textFile(dataset_file)\n",
        "    header = rdd.first()\n",
        "    return rdd.filter(lambda line: line != header and line.strip() != \"\")\n",
        "\n",
        "# Função reducer para somar contagens\n",
        "def reducer_counts(count1, count2):\n",
        "    return count1 + count2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9u7d2EMNj6w"
      },
      "source": [
        "#**Basics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90ohC4ZCozDa"
      },
      "outputs": [],
      "source": [
        "# Basics - outputs\n",
        "output_basic_raceColor = \"output_basic_raceColor\"\n",
        "output_basic_gender = \"output_basic_gender\"\n",
        "output_basic_states = \"output_basic_states\"\n",
        "output_basic_occupationsDistributions = \"output_basic_occupationsDistributions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpawbBONP5Li"
      },
      "source": [
        "### Record count by state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função mapper para extrair o valor de ESTADO e retornar um par (chave, valor)\n",
        "def mapper(line):\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) >= 2:\n",
        "        estado = columns[1].strip('\"')\n",
        "        return (estado, 1)\n",
        "    else:\n",
        "        return (\"Invalid\", 0)\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"ContagemEstado\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    # Mapeamento, filtragem e redução\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] != \"Invalid\")\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    # Ordena os resultados por valor crescente\n",
        "    sorted_rdd = reducer_rdd.sortBy(lambda x: x[1], ascending=True)\n",
        "\n",
        "    # Salva o resultado no diretório de saída\n",
        "    remove_dir(output_dir)\n",
        "    sorted_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_basic_states)"
      ],
      "metadata": {
        "id": "xr5645WNvmN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LhLBQFnPRH6"
      },
      "source": [
        "### Record count by Race/Color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4-kuMHbSxor"
      },
      "outputs": [],
      "source": [
        "# Função mapper para extrair o valor de RACACOR e retornar um par (chave, valor)\n",
        "def mapper(line):\n",
        "    # Divide a linha em colunas\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) >= 7:\n",
        "        racacor = columns[7].strip('\"')\n",
        "        return (racacor, 1)\n",
        "    else:\n",
        "      return (\"Invalid\", 0)\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"CountRaceColor\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] not in [(\"Invalid\", 0), (\"Error\", 0)])\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    # Ordena os resultados por valor decrescente\n",
        "    sorted_rdd = reducer_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    sorted_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_basic_raceColor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezKF99LpPnFC"
      },
      "source": [
        "### Record count by gender"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função mapper para extrair o valor de GÊNERO e retornar um par (chave, valor)\n",
        "def mapper(line):\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) >= 7:\n",
        "        gender = columns[6].strip('\"')\n",
        "        return (gender, 1)\n",
        "    else:\n",
        "        return (\"Invalid\", 0)\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = SparkContext(appName=\"ContagemSexo\")  # Configura o Spark\n",
        "\n",
        "    # Carrega os dados\n",
        "    filtered_rdd = sc.textFile(dataset_file)\n",
        "\n",
        "    # Mapeamento, filtragem e redução\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] != \"Invalid\")\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    # Ordena os resultados por valor decrescente\n",
        "    sorted_rdd = reducer_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "    # Salva o resultado no diretório de saída\n",
        "    remove_dir(output_dir)\n",
        "    sorted_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_basic_gender)\n"
      ],
      "metadata": {
        "id": "iWRC5ViO7rdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affQLlxHP2ra"
      },
      "source": [
        "### Record count by the distribution of occupations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IADbX1_gdj8y"
      },
      "outputs": [],
      "source": [
        "# Função mapper para extrair o valor de OCUPAÇÃO e retornar um par (chave, valor)\n",
        "def mapper(line):\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) >= 12:\n",
        "        ocup = columns[12].strip('\"')\n",
        "        return (ocup, 12)\n",
        "    else:\n",
        "        return (\"Invalid\", 0)\n",
        "\n",
        "# Função reducer para somar as ocorrências de cada estado\n",
        "    sorted_rdd = reducer_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "    # Salva o resultado no diretório de saída\n",
        "    remove_dir(output_dir)\n",
        "    sorted_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_basic_occupationsDistributions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE91n14tNz8h"
      },
      "source": [
        "#**Intermediaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0MQmt00o7YB"
      },
      "outputs": [],
      "source": [
        "# intermediaries - outputs\n",
        "output_intermediary_genderYear = \"output_intermediary_genderYear\"\n",
        "output_intermediary_genderState = \"output_intermediary_genderState\"\n",
        "output_intermediary_occupationsDistributionsState = \"output_intermediary_occupationsDistributionsState\"\n",
        "output_intermediary_dateOfBirthState = \"output_intermediary_dateOfBirthState\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7-N4A0tQQah"
      },
      "source": [
        "### Record count gender by year"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função mapper para processar cada linha\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)  # Divide a linha em colunas\n",
        "    if len(columns) >= 6:  # Verifica se há colunas suficientes\n",
        "        state = columns[2].strip('\"')\n",
        "        date_of_birth = columns[6].strip('\"')\n",
        "        return ((state, date_of_birth), 1)  # Retorna a chave (estado, data) e contagem 1\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\"), 0)  # Retorna um par inválido se não houver colunas suficientes\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"Count_GenderYea\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] not in [(\"Invalid\", \"Invalid\"), (\"Error\", \"Error\")])\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    reducer_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_intermediary_genderYear)\n"
      ],
      "metadata": {
        "id": "YlNsOgBtbLIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVxA7KH5QVnS"
      },
      "source": [
        "### Record count gender by state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função mapper para processar cada linha\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)  # Divide a linha em colunas\n",
        "    if len(columns) >= 6:  # Verifica se há colunas suficientes\n",
        "        state = columns[1].strip('\"')\n",
        "        date_of_birth = columns[6].strip('\"')\n",
        "        return ((state, date_of_birth), 1)  # Retorna a chave (estado, data) e contagem 1\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\"), 0)  # Retorna um par inválido se não houver colunas suficientes\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"Count_DateOfBirthState\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] not in [(\"Invalid\", \"Invalid\"), (\"Error\", \"Error\")])\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    reducer_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_intermediary_genderState)"
      ],
      "metadata": {
        "id": "QcNlPu0Gbtht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrjwm0pQZYb"
      },
      "source": [
        "### Record count occupations distribuitions by state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuUY8PMTdo6q"
      },
      "outputs": [],
      "source": [
        "# Função auxiliar para dividir a linha em colunas\n",
        "def split_csv_line(line):\n",
        "    return line.split(\",\")\n",
        "\n",
        "# Função mapper para processar cada linha\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)  # Divide a linha em colunas\n",
        "    if len(columns) >= 13:  # Verifica se há colunas suficientes\n",
        "        estado = columns[1].strip('\"')\n",
        "        ocupacao = columns[12].strip('\"')\n",
        "        return ((estado, ocupacao), 1)  # Retorna a chave (estado, ocupacao) e contagem 1\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\"), 0)  # Retorna um par inválido se não houver colunas suficientes\n",
        "\n",
        "# Função principal para processar o arquivo\n",
        "def process_file(dataset_file, output_dir):\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    # Carrega o arquivo, aplica o mapper e reduz por chave\n",
        "    data = sc.textFile(dataset_file)\n",
        "    reducer_rdd = (\n",
        "        data.map(mapper)\n",
        "        .filter(lambda x: x[0] != (\"Invalid\", \"Invalid\"))  # Filtra entradas inválidas\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "\n",
        "    # Remove o diretório de saída, se necessário\n",
        "    def remove_dir(directory):\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(directory)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    reducer_rdd.saveAsTextFile(output_dir)\n",
        "    sc.stop()\n",
        "\n",
        "process_file(dataset_file, output_intermediary_ocupacaoEstado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgsLOQA4QhIa"
      },
      "source": [
        "### Record count date of birth by state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oalgX_OGdpMp"
      },
      "outputs": [],
      "source": [
        "# Função mapper para processar cada linha\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)  # Divide a linha em colunas\n",
        "    if len(columns) >= 6:  # Verifica se há colunas suficientes\n",
        "        state = columns[1].strip('\"')\n",
        "        date_of_birth = columns[5].strip('\"')\n",
        "        return ((state, date_of_birth), 1)  # Retorna a chave (estado, data) e contagem 1\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\"), 0)  # Retorna um par inválido se não houver colunas suficientes\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"Count_DateOfBirthState\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    mapper_rdd = filtered_rdd.map(mapper).filter(lambda x: x[0] not in [(\"Invalid\", \"Invalid\"), (\"Error\", \"Error\")])\n",
        "    reducer_rdd = mapper_rdd.reduceByKey(reducer_counts)\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    reducer_rdd.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_intermediary_dateOfBirthState)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaP5NSWWN2jp"
      },
      "source": [
        "#**Advanced**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGB1SvRMpGmI"
      },
      "outputs": [],
      "source": [
        "# Advanced - outputs\n",
        "output_advanced_assistanceCorrelation = \"output_advanced_assistanceCorrelation\"\n",
        "output_advanced_occupationCause = \"output_advanced_occupationCause\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlOXUa8Q_rR"
      },
      "source": [
        "### Assistance correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1X0X70vdqzJ"
      },
      "outputs": [],
      "source": [
        "# Função de mapeamento (corrigido para tratar campos CSV corretamente)\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)\n",
        "    # Verifica se o número de colunas é suficiente\n",
        "    if len(columns) >= 14:\n",
        "        # Limpa e organiza as colunas necessárias\n",
        "        codmunres = columns[13].strip('\"')\n",
        "        ano = columns[1].strip('\"')\n",
        "        estciv = columns[10].strip('\"')\n",
        "        assistmed = columns[8].strip('\"')\n",
        "\n",
        "        # Verifica se a assistência médica está presente\n",
        "        assistance = 1 if assistmed.lower() == \"sim\" else 0\n",
        "        return ((codmunres, ano, estciv), (assistance, 1))\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\", \"Invalid\"), (0, 0))  # Ignora linhas mal formatadas\n",
        "\n",
        "# Função de redução\n",
        "def reducer(value1, value2):\n",
        "    total_assistencias = value1[0] + value2[0]\n",
        "    total_ocorrencias = value1[1] + value2[1]\n",
        "    return (total_assistencias, total_ocorrencias)\n",
        "\n",
        "# Função para calcular o percentual de assistência\n",
        "def calculate_percentage(record):\n",
        "    key, (total_assistencias, total_ocorrencias) = record\n",
        "    if total_ocorrencias > 0:\n",
        "        percentual = (total_assistencias / total_ocorrencias) * 100\n",
        "    else:\n",
        "        percentual = 0.0\n",
        "    return (key, f\"{percentual:.2f}%\")\n",
        "\n",
        "# Processa o arquivo de entrada e salva o resultado no diretório de saída\n",
        "def process_file(dataset_file, output_dir):\n",
        "    global sc  # Para usar o contexto do Spark globalmente\n",
        "    sc = create_spark_context(\"AssistenciaMedicaPercentage\")  # Configura o Spark\n",
        "\n",
        "    filtered_rdd = load_data(dataset_file)  # Carrega os dados\n",
        "\n",
        "    job_mapper = filtered_rdd.map(mapper).filter(lambda x: x[0] not in [(\"Invalid\", \"Invalid\", \"Invalid\"), (\"Error\", \"Error\", \"Error\")])\n",
        "    job_reducer = job_mapper.reduceByKey(reducer)\n",
        "\n",
        "    # Calcular o percentual de assistência\n",
        "    percentages = job_reducer.map(calculate_percentage)\n",
        "\n",
        "    # Remover o diretório de saída, se já existir, e salvar o resultado\n",
        "    remove_dir(output_dir)\n",
        "    percentages.saveAsTextFile(output_dir)\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "# Chamada da função com os parâmetros necessários\n",
        "process_file(dataset_file, output_advanced_assistanceCorrelation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bprhy102RQgb"
      },
      "source": [
        "### Occupation and cause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_MGuIJ-drox"
      },
      "outputs": [],
      "source": [
        "# Função auxiliar para dividir a linha em colunas\n",
        "def split_csv_line(line):\n",
        "    return line.split(\",\")\n",
        "\n",
        "# Função de mapeamento para extrair estado, ocupacao e causabas\n",
        "def mapper(line):\n",
        "    columns = split_csv_line(line)\n",
        "    # Verifica se o número de colunas é suficiente\n",
        "    if len(columns) >= 15:\n",
        "        # Limpa e organiza as colunas necessárias\n",
        "        estado = columns[1].strip('\"')\n",
        "        ocupacao = columns[12].strip('\"')\n",
        "        causabas = columns[14].strip('\"')\n",
        "        return ((estado, ocupacao, causabas), 1)  # Retorna a chave (estado, ocupacao, causabas) e contagem 1\n",
        "    else:\n",
        "        return ((\"Invalid\", \"Invalid\", \"Invalid\"), 0)  # Par inválido se não houver colunas suficientes\n",
        "\n",
        "# Função principal para processar o arquivo\n",
        "def process_file(dataset_file, output_dir):\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    # Carrega o arquivo, aplica o mapper e reduz por chave\n",
        "    data = sc.textFile(dataset_file)\n",
        "    reducer_rdd = (\n",
        "        data.map(mapper)\n",
        "        .filter(lambda x: x[0] != (\"Invalid\", \"Invalid\", \"Invalid\"))  # Filtra entradas inválidas\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "\n",
        "    # Remove o diretório de saída, se necessário\n",
        "    def remove_dir(directory):\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(directory)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    remove_dir(output_dir)\n",
        "    reducer_rdd.saveAsTextFile(output_dir)\n",
        "    sc.stop()\n",
        "\n",
        "process_file(dataset_file, output_advanced_ocupacaoDistribuicao)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}